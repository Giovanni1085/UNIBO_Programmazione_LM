{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Web Scraping and APIs\n",
    "\n",
    "In this notebook, we learn how to scrape data from the Web and get an idea of what Application Programming Interfaces are (APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "**Web Scraping** is a technique for the extraction of information from websites by transforming unstructured data (HTML pages) into structured data (databases or spreadsheets). \n",
    "\n",
    "Even if scraping can be manually performed by a user, it is usually implemented using a **web crawler** (i.e., it is usually implemented as an automatic process). For larger scale scraping see, e.g., [Scrapy](https://scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is an alternative to using already available **API**s (Application Programming Interface), such as those provided by all the major platforms, like *Facebook*, *Google* and *Twitter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of HTML\n",
    "\n",
    "The **HyperText Markup Language (HTML)** is the standard **descriptive markup** language for web pages.\n",
    "\n",
    "\n",
    "- **Markup** language: a human-readable, explicit system for annotating the content of a document. Markdown is another markup language.\n",
    "\n",
    "\n",
    "- **Descriptive** markup languages (e.g. HTML, XML) are used to annotate the structure or the contents of a document, as opposed to **procedural** markup languages (e.g. TEX, Postscript), whose main goal is to describe how a document should be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML provides a means to annotate the <strong>structural</strong> elements of documents like headings, paragraphs, lists, links, images, quotes, tables and so forth. Similarly, even if with fewer options, does Markdown.\n",
    "\n",
    "HTML tags **do not mark the logical structure** of a document, but only its format (e.g. *this is a table*, *this is a h3-type heading*...). It is up to the browser to then use HTML (plus other information, such as *Cascading Style Sheets*), to render a webpage appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML markup relies on a **fixed inventory of tags**, written by using angle brackets. Some tags, e.g. `<p>...</p>`, surround the marked text, and may include subelements. Other tags, e.g. `<br>` or `<img>` introduce content directly.\n",
    "\n",
    "The following is an example of a web page:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>The Adventures of Pinocchio</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2>Carlo Collodi</h2>\n",
    "    <h1>The Adventures of Pinocchio</h1>\n",
    "    <hr>\n",
    "    <h4>CHAPTER 1</h4>\n",
    "    <br>\n",
    "    <p><i>How it happened that Mastro Cherry, carpenter, found a piece of wood that wept and laughed like a child</i></p>\n",
    "    <br>\n",
    "    <p>Centuries ago there lived--</p>\n",
    "    <p>\"A king!\" my little readers will say immediately.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following notes are roughly based on the **Chapters 1-3** of: Mitchell, R. (2015). [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do), O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules and Packages Required for Web Scraping\n",
    "\n",
    "**BeautifulSoup**: this library defines [classes and functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to pull data (e.g. table, lists, paragraphs) out of HTML and XML files. It provides idiomatic ways of navigating, searching, and modifying the parse tree.\n",
    "\n",
    "\n",
    "**lxml**: to function, BeautifulSoup relies on external HTML-XML parsers. Many options are available, among which the html5lib's and the Python's built-in parsers. We'll rely on the [lxml](http://lxml.de/)'s parser, due to its high performance, reliability and flexibility.\n",
    "\n",
    "\n",
    "**Urllib**: BeautifulSoup does not fetch the web page for us. To do this, we'll rely on the [Urllib](https://docs.python.org/3.11/library/urllib.html#module-urllib) module available in the Python Standard Library, that implements classes and functions which help in opening URLs (authentication, redirections, cookies and so on). We will see another option, **requests**, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Parse an HTML page\n",
    "\n",
    "`urllib.request.urlopen()` allows us to retrieve our target HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the page doesn't exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's handle this properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except urllib.request.URLError as e:\n",
    "    pass # code your plan B here\n",
    "except urllib.request.URLError as e:\n",
    "    raise # raise any other exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `BeautifulSoup()` in conjunction with `lxml` to parse out `html` page and store it in the Beautiful Soup format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to to the following:\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "soup_page1 = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's scrape another couple of pages we'll need in our examples\n",
    "soup_page3 = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/page3.html\"), \"lxml\")\n",
    "soup_wap = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Let's look at the nested structure of the page\n",
    "\n",
    "The `prettify()` method allows us to have a look at the structure of the HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_page1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Let's play with a HTML tag\n",
    "\n",
    "The notation `soup.<tag>` allows us to retrieve the content marked by a tag (opening and closing tags included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the first \"<div>\" tag is nested two layers deep (html → body → div).\n",
    "soup_page1.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text is the only thing you're interested into, well, the `soup.<tag>.string` method comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page1.div.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML markup generated by Beautiful Soup can be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's change the content of our div\n",
    "soup_page1.div.string = \"this content has been changed\"\n",
    "# let's change the name of the tag\n",
    "soup_page1.div.name = \"new_div\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest use, the `find()` method is an alternative to the `soup.<tag>` notation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page1.find(\"new_div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page1.new_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but this function allows for the searching of nodes by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_wap.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find(\"span\", attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of an attribute for a given tag instance can be retrieved by using the `get(\"ATTRIBUTE\")` method. For instance, if we want to retrieve the URL of an image we can extract the `src` value from the corresponding `<img>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page3.img.get(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know all the attibutes associated with a given tag, the `attrs` method is convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_page3.img.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by returning a dictionary, it is easy to see how \"attrs\" can be used as an alternative to \"get()\"\n",
    "soup_page3.img.attrs[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you fancy another way to do the same thing...\n",
    "soup_page3.img[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with multiple HTML tags at once\n",
    "\n",
    "When the same tag is used multiple time in the same page, however, both the `soup.<tag>` notation and the `find()` method allow you to access **only one instance** (i.e. the first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_wap.prettify())[180:1190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the **sequence of all the instances of a tag** in a file, we can use the `find_all()` method (previously known as `findAll()` and `findChildren()` in BS 3 and BS 2, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find_all(\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The `find_all()` method as well allows for  the extraction of  all tags by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_wap.find_all(\"span\",  attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling\n",
    "\n",
    "Web Crawlers are softwares designed to collect pages from the Web. In essence, they recursively implement the following steps: \n",
    "\n",
    "- they start by retrieving the page content for an URL \n",
    "\n",
    "\n",
    "- they then parse it to retrieve other URLs of interest\n",
    "\n",
    "\n",
    "- they then focus on these new URLs, for each of which they repeat the whole process, ad infinitum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you want to crawl and **entire site**:\n",
    "\n",
    "- start with a top-level page\n",
    "\n",
    "\n",
    "- parse the page (retrieve the data your application need) and extract all the internal links, by ignoring already visited URLs\n",
    "\n",
    "\n",
    "- for each new link, move to the corresponding page and repeat the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Random walk through Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our starting page URL, fetch it and parse its HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_page = urlopen(\"https://en.wikipedia.org/wiki/Chris_Cornell\")\n",
    "soup = BeautifulSoup(starting_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it should be easy to extract all the links in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links are defined by <a> tag\n",
    "for link_element in soup.find_all(\"a\")[:10]:\n",
    "    print(link_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore all the \"a\" tags without an \"href\" attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link_element in [tag for tag in soup.find_all(\"a\") if 'href' in tag.attrs][:10]:\n",
    "    \n",
    "    url = link_element.attrs['href']\n",
    "    \n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is full of sidebar, footer, and header links that appear on every page, along with links to the category pages, talk pages, and other pages that do not contain different articles:\n",
    "\n",
    "```\n",
    "/wiki/Template_talk:Chris_Cornell\n",
    "```\n",
    "\n",
    "```\n",
    "#cite_note-147\n",
    "```\n",
    "\n",
    "Moreover, we don't want to visit pages outside of Wikipedia:\n",
    "\n",
    "```\n",
    "http://www.chriscornell.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant links have three thing in common:\n",
    "\n",
    "- they reside within the `div` with the `id` set to `bodyContent`\n",
    "\n",
    "\n",
    "- the URLs do not contain semicolons\n",
    "\n",
    "\n",
    "- the URLs begin with `/wiki/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "\n",
    "body = soup.find(\"div\", {\"id\": \"bodyContent\"})\n",
    "\n",
    "for link in body.find_all(\"a\", {'href': re_pattern}):\n",
    "\n",
    "    print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns the list of all the Wikipedia articles linked to our starting page. \n",
    "\n",
    "This is not enough, we want to be recursively repeat this process for all these links. That is, we need a function that takes as input a Wikipedia article URL of the form `/wiki/<Article_Name>` and returns a list of all linked articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(article_url):\n",
    "    \"\"\"\n",
    "    Retrieve all URLs on an English Wikipedia article page (e.g. /wiki/Amsterdam).\n",
    "    \n",
    "    This function needs a relative URL on the \n",
    "    http://en.wikipedia.org domain, such as '/wiki/Amsterdam'. \n",
    "    \n",
    "    Args:\n",
    "        article_url (str): URL of a website\n",
    "        \n",
    "    Returns:\n",
    "        bs4.element.ResultSet: bs link elements resultset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    page = urlopen(\"http://en.wikipedia.org\" + article_url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    body = soup.find(\"div\", {\"id\":\"bodyContent\"})\n",
    "    \n",
    "    re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "    \n",
    "    links = body.find_all(\"a\", href=re_pattern)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by calling it in a script that randomly select, for each iteration, a random link and that stops after 10 URLs have been retrieved (or when it bumps into a page without link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "links = get_links(\"/wiki/Chris_Cornell\")\n",
    "\n",
    "for _ in range(10):  # for testing purposes, we want to do this 10 times\n",
    "    if len(links) > 0:\n",
    "        new_article = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "        print(new_article)\n",
    "        \n",
    "        links = get_links(new_article)\n",
    "        \n",
    "    else:\n",
    "        print(\"No links in this page!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs\n",
    "\n",
    "An **Application Programming Interface** is a set of protocols that defines how software programs communicate among eachother. Without APIs, we have to scrape the Web or get the data directly. With APIs, we often can get structured data: it is a much more convenient way to work.\n",
    "\n",
    "APIs are a great option in that they implement extensively tested routines (**high reliability**). However, you should spend time in learning how they work and, in some cases, they don't allow you to access the piece of information you may need (**low flexibility**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # External package: https://requests.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tesla\"\n",
    "r = requests.get('https://www.google.com/search', params={'q': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.headers['content-type'])\n",
    "print(r.encoding)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using `requests` to query APIs? Easy using the param dictionary. Responses then follow the starndard format of the API (or you can request the one you like if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://api.github.com')\n",
    "\n",
    "# raw\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "\n",
    "Write code to retrieve the **number of students and year of foundation** of Italian universities by starting from the following Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_universities_in_Italy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "\n",
    "1. Inspect the Google search results page and understand how results are displayed.\n",
    "\n",
    "2. Use BeautifulSoup to get the link of the first 10 results of this search out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "\n",
    "Develop a simple crawler to download information of interest from Wikipedia. Customize it however you like.\n",
    "\n",
    "I recommend taking a look to the available APIs: https://meta.m.wikimedia.org/wiki/Research:Data#MediaWiki_API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
